{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a7033ce",
   "metadata": {},
   "source": [
    "\n",
    "-  Web Scraping: the automated process of extracting data from websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cd991e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 paginas scrapeadas con exito\n",
      "Vamos por los detalles de cada libro\n",
      "Scraping completo. Todos los detalles fueron agregados.\n",
      "Se guardaron 1000 libros\n",
      "{'Titulo': 'A Light in the Attic', 'Precio': 'Â£51.77', 'Disponibilidad': 'In stock', 'Link': 'https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html', 'Rating': 'Three', 'Descripcion': \"It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love th It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love that Silverstein. Need proof of his genius? RockabyeRockabye baby, in the treetopDon't you know a treetopIs no safe place to rock?And who put you up there,And your cradle, too?Baby, I think someone down here'sGot it in for you. Shel, you never sounded so good. ...more\", 'Categoria': 'Poetry', 'UPC': 'a897fe39b1053632'}\n"
     ]
    }
   ],
   "source": [
    "''' BLOQUE 1 - Scraping de todas las páginas del sitio'''\n",
    "\n",
    "# importar dependencias \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "import json\n",
    "\n",
    "# Se crea una lista vacía donde guardar todos los libros\n",
    "libros_registrados = []\n",
    "\n",
    "# Establecer la URL base\n",
    "url_inicio = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
    "url_base = \"http://books.toscrape.com/catalogue/\"\n",
    "\n",
    "next_page = url_inicio\n",
    "\n",
    "# Mientras haya una página siguiente:\n",
    "while next_page:\n",
    "\n",
    "    # Hacer request a la página actual\n",
    "    nuevo_request = requests.get(next_page)\n",
    "    # Parsear el HTML\n",
    "    nuevo_soup = BeautifulSoup(nuevo_request.text, 'lxml')\n",
    "\n",
    "    # Buscar todos los libros de esa página\n",
    "    libros = nuevo_soup.find_all('article', class_='product_pod')\n",
    "\n",
    "    # Por cada libro, extraer: título, precio, disponibilidad, link\n",
    "    for libro in libros:\n",
    "        titulos = libro.find('h3').a['title']\n",
    "        precios = libro.find('p', class_='price_color').text.strip()\n",
    "        disponibilidad = libro.find('p', class_='instock availability').text.strip()\n",
    "        a_links = libro.find('h3').a['href']\n",
    "        link_completo = urljoin(next_page,a_links)\n",
    "\n",
    "        # Guardar la info en la lista de libros\n",
    "        libros_registrados.append({\n",
    "        'Titulo': titulos,\n",
    "        'Precio': precios,\n",
    "        'Disponibilidad': disponibilidad,\n",
    "        'Link': link_completo\n",
    "        })\n",
    "    \n",
    "    # Buscar el enlace a la siguiente página (si existe)\n",
    "    boton_next = nuevo_soup.find('li', class_='next')\n",
    "    # condicion de boton next\n",
    "    if boton_next:\n",
    "        href_next = boton_next.a['href']\n",
    "        # Construir la URL completa de la siguiente página, y que continue el bucle anterior\n",
    "        next_page = urljoin(next_page, href_next)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Una vez fuera del bucle, tendremos una lista con todos los libros de todas las páginas\n",
    "print(f'50 paginas scrapeadas con exito')\n",
    "print(f'Vamos por los detalles de cada libro')\n",
    "\n",
    "'''\n",
    "BLOQUE 2.\n",
    "Recorrer cada libro de libros_registrados y visitar su página individual para obtener más detalles que no aparecen en la página principal.\n",
    "Ej: Rating, categoría, descripción, UPC, tipo de producto, disponibilidad, etc.\n",
    "'''\n",
    "\n",
    "# 1. recorrer cada libro dentro de la lista de los libros registrados\n",
    "for libro in libros_registrados:\n",
    "    # 2. visitar su pagina individual\n",
    "    link_individual = libro['Link']\n",
    "    link_request = requests.get(link_individual)\n",
    "    link_individual_soup = BeautifulSoup(link_request.text, 'lxml')\n",
    "\n",
    "    #specs = link_individual_soup.find('div', class_='page_inner')\n",
    "    specs = link_individual_soup\n",
    "    \n",
    "    # 3. extraer detalles:\n",
    "\n",
    "    ## rating\n",
    "    rating_tag = specs.find('p', class_='star-rating')\n",
    "    #print(rating_tag)\n",
    "    if rating_tag:\n",
    "        rating = rating_tag['class'][1]     # Ej: 'star-rating Five'\n",
    "    else:\n",
    "        rating = 'Sin rating'\n",
    "\n",
    "    ## categoria\n",
    "    breadcrumb = specs.find('ul', class_='breadcrumb')\n",
    "    if breadcrumb:\n",
    "        categoria = breadcrumb.find_all('li')[2].text.strip()\n",
    "    else:\n",
    "        categoria = 'Sin categoria'\n",
    "\n",
    "    ## descripcion\n",
    "    descripcion_tag = specs.find('div', id='product_description')\n",
    "    #print(descripcion_tag)\n",
    "    if descripcion_tag:\n",
    "        descripcion = descripcion_tag.find_next_sibling('p').text.strip()\n",
    "    else:\n",
    "        descripcion = 'Sin descripcion'\n",
    "\n",
    "    ## UPC (codigo universal del producto)\n",
    "    tabla_informacion = specs.find('table', class_='table table-striped')\n",
    "    if tabla_informacion:\n",
    "        upc = tabla_informacion.find('tr').find('td').text \n",
    "    else:\n",
    "        upc = 'Sin UPC' \n",
    "\n",
    "    # Actualiza el diccionario\n",
    "    libro['Rating'] = rating\n",
    "    libro['Descripcion'] = descripcion\n",
    "    libro['Categoria'] = categoria\n",
    "    libro['UPC'] = upc\n",
    "\n",
    "print(\"Scraping completo. Todos los detalles fueron agregados.\")\n",
    "\n",
    "# Guardar en JSON\n",
    "with open('libros_scrapeados.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(libros_registrados, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Leer desde JSON\n",
    "with open('libros_scrapeados.json', 'r', encoding='utf-8') as f:\n",
    "    datos = json.load(f)\n",
    "    print(f'Se guardaron {len(datos)} libros')\n",
    "\n",
    "    # Mostrar el primero para validar estructura\n",
    "    print(datos[0])  \n",
    "\n",
    "# Abrir automáticamente el archivo (solo en Windows)\n",
    "import os\n",
    "os.startfile('libros_scrapeados.json')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
