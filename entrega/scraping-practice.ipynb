{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a7033ce",
   "metadata": {},
   "source": [
    "## Archivo para practicar Web Scraping basico\n",
    "\n",
    "-  Web Scraping: the automated process of extracting data from websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaad07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Titulo': 'A Light in the ...', 'Precio': 'Â£51.77', 'Disponibilidad': 'In stock', 'Link': 'catalogue/a-light-in-the-attic_1000/index.html'}, {'Titulo': 'Tipping the Velvet', 'Precio': 'Â£53.74', 'Disponibilidad': 'In stock', 'Link': 'catalogue/tipping-the-velvet_999/index.html'}, {'Titulo': 'Soumission', 'Precio': 'Â£50.10', 'Disponibilidad': 'In stock', 'Link': 'catalogue/soumission_998/index.html'}, {'Titulo': 'Sharp Objects', 'Precio': 'Â£47.82', 'Disponibilidad': 'In stock', 'Link': 'catalogue/sharp-objects_997/index.html'}, {'Titulo': 'Sapiens: A Brief History ...', 'Precio': 'Â£54.23', 'Disponibilidad': 'In stock', 'Link': 'catalogue/sapiens-a-brief-history-of-humankind_996/index.html'}, {'Titulo': 'The Requiem Red', 'Precio': 'Â£22.65', 'Disponibilidad': 'In stock', 'Link': 'catalogue/the-requiem-red_995/index.html'}, {'Titulo': 'The Dirty Little Secrets ...', 'Precio': 'Â£33.34', 'Disponibilidad': 'In stock', 'Link': 'catalogue/the-dirty-little-secrets-of-getting-your-dream-job_994/index.html'}, {'Titulo': 'The Coming Woman: A ...', 'Precio': 'Â£17.93', 'Disponibilidad': 'In stock', 'Link': 'catalogue/the-coming-woman-a-novel-based-on-the-life-of-the-infamous-feminist-victoria-woodhull_993/index.html'}, {'Titulo': 'The Boys in the ...', 'Precio': 'Â£22.60', 'Disponibilidad': 'In stock', 'Link': 'catalogue/the-boys-in-the-boat-nine-americans-and-their-epic-quest-for-gold-at-the-1936-berlin-olympics_992/index.html'}, {'Titulo': 'The Black Maria', 'Precio': 'Â£52.15', 'Disponibilidad': 'In stock', 'Link': 'catalogue/the-black-maria_991/index.html'}, {'Titulo': 'Starving Hearts (Triangular Trade ...', 'Precio': 'Â£13.99', 'Disponibilidad': 'In stock', 'Link': 'catalogue/starving-hearts-triangular-trade-trilogy-1_990/index.html'}, {'Titulo': \"Shakespeare's Sonnets\", 'Precio': 'Â£20.66', 'Disponibilidad': 'In stock', 'Link': 'catalogue/shakespeares-sonnets_989/index.html'}, {'Titulo': 'Set Me Free', 'Precio': 'Â£17.46', 'Disponibilidad': 'In stock', 'Link': 'catalogue/set-me-free_988/index.html'}, {'Titulo': \"Scott Pilgrim's Precious Little ...\", 'Precio': 'Â£52.29', 'Disponibilidad': 'In stock', 'Link': 'catalogue/scott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html'}, {'Titulo': 'Rip it Up and ...', 'Precio': 'Â£35.02', 'Disponibilidad': 'In stock', 'Link': 'catalogue/rip-it-up-and-start-again_986/index.html'}, {'Titulo': 'Our Band Could Be ...', 'Precio': 'Â£57.25', 'Disponibilidad': 'In stock', 'Link': 'catalogue/our-band-could-be-your-life-scenes-from-the-american-indie-underground-1981-1991_985/index.html'}, {'Titulo': 'Olio', 'Precio': 'Â£23.88', 'Disponibilidad': 'In stock', 'Link': 'catalogue/olio_984/index.html'}, {'Titulo': 'Mesaerion: The Best Science ...', 'Precio': 'Â£37.59', 'Disponibilidad': 'In stock', 'Link': 'catalogue/mesaerion-the-best-science-fiction-stories-1800-1849_983/index.html'}, {'Titulo': 'Libertarianism for Beginners', 'Precio': 'Â£51.33', 'Disponibilidad': 'In stock', 'Link': 'catalogue/libertarianism-for-beginners_982/index.html'}, {'Titulo': \"It's Only the Himalayas\", 'Precio': 'Â£45.17', 'Disponibilidad': 'In stock', 'Link': 'catalogue/its-only-the-himalayas_981/index.html'}]\n"
     ]
    }
   ],
   "source": [
    "'''' Ejercicio 1 - Scraping de la informacion de los libros de la pagina principal\n",
    "1. Hacer una request a la URL principal (http://books.toscrape.com)\n",
    "2. Parsear el HTML con BeautifulSoup\n",
    "3. Buscar todos los elementos que contienen los libros (usualmente <article class=\"product_pod\">)\n",
    "4. Para cada libro:\n",
    "    a. Extraer el título\n",
    "    b. Extraer el precio\n",
    "    c. Extraer la disponibilidad (en stock o no)\n",
    "    d. Extraer el enlace al detalle del libro (href parcial)\n",
    "    e. Guardar esta info en una lista o dict\n",
    "'''\n",
    "# importar dependencias \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "books_page = requests.get(\"https://books.toscrape.com/\").text\n",
    "soup = BeautifulSoup(books_page, 'lxml')\n",
    "#print(books_page)\n",
    "#print(soup)\n",
    "\n",
    "libros = soup.find_all('article', class_='product_pod')\n",
    "libros_informacion = []\n",
    "registro_libros = {}\n",
    "contador = 0\n",
    "for libro in libros:\n",
    "    contador += 1\n",
    "    # 1. titulo\n",
    "    titulos = libro.find('h3').text\n",
    "    #print(titulos)\n",
    "\n",
    "    # 2. precio\n",
    "    precio = libro.find('p', class_='price_color').text\n",
    "    #print(precio)\n",
    "\n",
    "    # 3. disponibilidad\n",
    "    disponibilidad = libro.find('p', class_='instock availability').get_text(separator='\\n', strip=True)\n",
    "    #print(disponibilidad)\n",
    "\n",
    "    # 4. enlace al detalle del libro (href)\n",
    "    #div_links = libro.find('div', class_='image_container').a\n",
    "    #links = div_links.get('href')\n",
    "    a_links = libro.find('a')\n",
    "    links = a_links.get('href')\n",
    "    #print(links)\n",
    "\n",
    "    # 5. guardar info en una lista\n",
    "    libros_informacion.append({\n",
    "        'Titulo': titulos,\n",
    "        'Precio': precio,\n",
    "        'Disponibilidad': disponibilidad,\n",
    "        'Link': links\n",
    "    })\n",
    "\n",
    "    # registro_libros[f'Titulo {contador}'] = titulos\n",
    "    # registro_libros[f'Precio {contador}'] = precio\n",
    "    # registro_libros[f'Disponibilidad {contador}'] = disponibilidad\n",
    "    # registro_libros[f'Link {contador}'] = links\n",
    "    \n",
    "    # 6. Mostrar en pantalla ordenado\n",
    "    # print(\n",
    "    #     f'Titulo {contador}: {titulos}',\n",
    "    #     f'Precio: {precio}',\n",
    "    #     f'Disponibilidad: {disponibilidad}',\n",
    "    #     f'Link de referencia: {links}'\n",
    "    # )\n",
    "    # print()\n",
    "\n",
    "print(libros_informacion)\n",
    "#print(registro_libros)\n",
    "\n",
    "######## encontrar el link de 'Books' (enfocado en la parte de categorias)\n",
    "#libros = soup.find_all('div', class_='side_categories')\n",
    "#libros = soup.find_all('ul', 'nav nav-list')\n",
    "#print(libros)\n",
    "\n",
    "# for libro in libros:\n",
    "#     primera_li = libro.find('li').a\n",
    "#     #print(primera_li)\n",
    "#     link = primera_li.get('href')\n",
    "#     print(link)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc33a7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Ejercicio 2 - Scraping de todas las páginas del sitio\n",
    "1. Crear una lista vacía donde vas a guardar todos los libros\n",
    "2. Establecer la URL base (http://books.toscrape.com/catalogue/page-1.html)\n",
    "3. Mientras haya una página siguiente:\n",
    "    a. Hacer request a la página actual\n",
    "    b. Parsear el HTML\n",
    "    c. Buscar todos los libros de esa página\n",
    "    d. Por cada libro, extraer: título, precio, disponibilidad, link\n",
    "    e. Guardar la info en la lista de libros\n",
    "    f. Buscar el enlace a la siguiente página (si existe)\n",
    "    g. Construir la URL completa de la siguiente página\n",
    "    h. Volver al paso 3 con la nueva URL\n",
    "4. Al final, tendrás una lista con todos los libros de todas las páginas'''\n",
    "\n",
    "# importar dependencias \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "\n",
    "books_page = requests.get(\"https://books.toscrape.com/\")\n",
    "soup = BeautifulSoup(books_page.text, 'lxml')\n",
    "\n",
    "libros_registrados = []\n",
    "\n",
    "url_inicio = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
    "url_base = \"http://books.toscrape.com/catalogue/\"\n",
    "\n",
    "next_page = url_inicio\n",
    "\n",
    "\n",
    "while next_page:\n",
    "\n",
    "    nuevo_request = requests.get(next_page)\n",
    "    nuevo_soup = BeautifulSoup(nuevo_request.text, 'lxml')\n",
    "\n",
    "    libros = nuevo_soup.find_all('article', class_='product_pod')\n",
    "    for libro in libros:\n",
    "        titulos = libro.find('h3').a['title']\n",
    "        precios = libro.find('p', class_='price_color').text.strip()\n",
    "        disponibilidad = libro.find('p', class_='instock availability').text.strip()\n",
    "        a_links = libro.find('h3').a['href']\n",
    "        link_completo = urljoin(next_page,a_links)\n",
    "\n",
    "        libros_registrados.append({\n",
    "        'Titulo': titulos,\n",
    "        'Precio': precios,\n",
    "        'Disponibilidad': disponibilidad,\n",
    "        'Link': link_completo\n",
    "        })\n",
    "        \n",
    "    # condicion de next\n",
    "    boton_next = nuevo_soup.find('li', class_='next')\n",
    "    if boton_next:\n",
    "        href_next = boton_next.a['href']\n",
    "        next_page = urljoin(next_page, href_next)\n",
    "    else:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8b06f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar el registro de libros\n",
    "print(libros_registrados)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cd991e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 paginas scrapeadas con exito\n",
      "Vamos por los detalles de cada libro\n",
      "Scraping completo. Todos los detalles fueron agregados.\n",
      "Se guardaron 1000 libros\n",
      "{'Titulo': 'A Light in the Attic', 'Precio': 'Â£51.77', 'Disponibilidad': 'In stock', 'Link': 'https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html', 'Rating': 'Three', 'Descripcion': \"It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love th It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love that Silverstein. Need proof of his genius? RockabyeRockabye baby, in the treetopDon't you know a treetopIs no safe place to rock?And who put you up there,And your cradle, too?Baby, I think someone down here'sGot it in for you. Shel, you never sounded so good. ...more\", 'Categoria': 'Poetry', 'UPC': 'a897fe39b1053632'}\n"
     ]
    }
   ],
   "source": [
    "''' BLOQUE 1 - Scraping de todas las páginas del sitio'''\n",
    "\n",
    "# importar dependencias \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "import json\n",
    "\n",
    "# Se crea una lista vacía donde guardar todos los libros\n",
    "libros_registrados = []\n",
    "\n",
    "# Establecer la URL base\n",
    "url_inicio = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
    "url_base = \"http://books.toscrape.com/catalogue/\"\n",
    "\n",
    "next_page = url_inicio\n",
    "\n",
    "# Mientras haya una página siguiente:\n",
    "while next_page:\n",
    "\n",
    "    # Hacer request a la página actual\n",
    "    nuevo_request = requests.get(next_page)\n",
    "    # Parsear el HTML\n",
    "    nuevo_soup = BeautifulSoup(nuevo_request.text, 'lxml')\n",
    "\n",
    "    # Buscar todos los libros de esa página\n",
    "    libros = nuevo_soup.find_all('article', class_='product_pod')\n",
    "\n",
    "    # Por cada libro, extraer: título, precio, disponibilidad, link\n",
    "    for libro in libros:\n",
    "        titulos = libro.find('h3').a['title']\n",
    "        precios = libro.find('p', class_='price_color').text.strip()\n",
    "        disponibilidad = libro.find('p', class_='instock availability').text.strip()\n",
    "        a_links = libro.find('h3').a['href']\n",
    "        link_completo = urljoin(next_page,a_links)\n",
    "\n",
    "        # Guardar la info en la lista de libros\n",
    "        libros_registrados.append({\n",
    "        'Titulo': titulos,\n",
    "        'Precio': precios,\n",
    "        'Disponibilidad': disponibilidad,\n",
    "        'Link': link_completo\n",
    "        })\n",
    "    \n",
    "    # Buscar el enlace a la siguiente página (si existe)\n",
    "    boton_next = nuevo_soup.find('li', class_='next')\n",
    "    # condicion de boton next\n",
    "    if boton_next:\n",
    "        href_next = boton_next.a['href']\n",
    "        # Construir la URL completa de la siguiente página, y que continue el bucle anterior\n",
    "        next_page = urljoin(next_page, href_next)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Una vez fuera del bucle, tendremos una lista con todos los libros de todas las páginas\n",
    "print(f'50 paginas scrapeadas con exito')\n",
    "print(f'Vamos por los detalles de cada libro')\n",
    "\n",
    "'''\n",
    "BLOQUE 2.\n",
    "Recorrer cada libro de libros_registrados y visitar su página individual para obtener más detalles que no aparecen en la página principal.\n",
    "Ej: Rating, categoría, descripción, UPC, tipo de producto, disponibilidad, etc.\n",
    "'''\n",
    "\n",
    "# 1. recorrer cada libro dentro de la lista de los libros registrados\n",
    "for libro in libros_registrados:\n",
    "    # 2. visitar su pagina individual\n",
    "    link_individual = libro['Link']\n",
    "    link_request = requests.get(link_individual)\n",
    "    link_individual_soup = BeautifulSoup(link_request.text, 'lxml')\n",
    "\n",
    "    #specs = link_individual_soup.find('div', class_='page_inner')\n",
    "    specs = link_individual_soup\n",
    "    \n",
    "    # 3. extraer detalles:\n",
    "\n",
    "    ## rating\n",
    "    rating_tag = specs.find('p', class_='star-rating')\n",
    "    #print(rating_tag)\n",
    "    if rating_tag:\n",
    "        rating = rating_tag['class'][1]     # Ej: 'star-rating Five'\n",
    "    else:\n",
    "        rating = 'Sin rating'\n",
    "\n",
    "    ## categoria\n",
    "    breadcrumb = specs.find('ul', class_='breadcrumb')\n",
    "    if breadcrumb:\n",
    "        categoria = breadcrumb.find_all('li')[2].text.strip()\n",
    "    else:\n",
    "        categoria = 'Sin categoria'\n",
    "\n",
    "    ## descripcion\n",
    "    descripcion_tag = specs.find('div', id='product_description')\n",
    "    #print(descripcion_tag)\n",
    "    if descripcion_tag:\n",
    "        descripcion = descripcion_tag.find_next_sibling('p').text.strip()\n",
    "    else:\n",
    "        descripcion = 'Sin descripcion'\n",
    "\n",
    "    ## UPC (codigo universal del producto)\n",
    "    tabla_informacion = specs.find('table', class_='table table-striped')\n",
    "    if tabla_informacion:\n",
    "        upc = tabla_informacion.find('tr').find('td').text \n",
    "    else:\n",
    "        upc = 'Sin UPC' \n",
    "\n",
    "    # Actualiza el diccionario\n",
    "    libro['Rating'] = rating\n",
    "    libro['Descripcion'] = descripcion\n",
    "    libro['Categoria'] = categoria\n",
    "    libro['UPC'] = upc\n",
    "\n",
    "print(\"Scraping completo. Todos los detalles fueron agregados.\")\n",
    "\n",
    "# Guardar en JSON\n",
    "with open('libros_scrapeados.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(libros_registrados, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Leer desde JSON\n",
    "with open('libros_scrapeados.json', 'r', encoding='utf-8') as f:\n",
    "    datos = json.load(f)\n",
    "    print(f'Se guardaron {len(datos)} libros')\n",
    "\n",
    "    # Mostrar el primero para validar estructura\n",
    "    print(datos[0])  \n",
    "\n",
    "# Abrir automáticamente el archivo (solo en Windows)\n",
    "import os\n",
    "os.startfile('libros_scrapeados.json')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
